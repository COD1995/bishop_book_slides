---
marp: true
theme: notebook
math: katex
paginate: true

---
<!-- =========================================
     TITLE SLIDE (No Book Bind)
     ========================================= -->
<section>

  <!-- Main white paper w/ lines -->
  <div class="paper-container">
    <div class="lined-paper"></div>
    <div class="title-container">
        <h1>Pattern Recognition <br>and <br>Machine Learning</h1>
    </div>
    <div class="sticky-note">
    <h4>Chapter 1: Introduction</h4>
    <ul>
      <li>Polynomial Curve Fitting</li>
      <li>Probability Theory</li>
      <li>Model Selection</li>
      <li>The Curse of Dimensionality</li>
      <li>Decision Theory</li>
      <li>Information Theory</li>
    </ul>
    </div>
  </div>

  <!-- “Hello My Name Is” sticker at top-right -->
  <div class="hello-sticker">
    <div class="sticker-header">
      HELLO
      <br />
      MY NAME IS
    </div>
    <div class="sticker-name">
      Jue Guo
      <br />
      University at Buffalo, SUNY
    </div>
  </div>

</section>

---

<!-- _class: spiral-slide -->

<div class="notebook-container">
  <!-- The pink border around the white page -->
  <div class="page-border">
    <div class="sticky-tabs">
      <div class="tab-yellow"></div>
      <div class="tab-peach"></div>
    </div>
    <div class="white-page">
      <!-- Circle with the number -->
      <div class="circle-number">
        <span>01</span>
      </div>
    <h1 class="page-title">OVERVIEW</h1>
    <p class="subtitle">Introduction to Pattern Recognition</p>
    </div>
</div>

---

<section>
  <div class="body-container" markdown="1">
    <div class="notepad-zone" markdown="1">
  
  ## History of Pattern Recognition

  - The problem of searching for patterns in data is a fundamental one and has a long and successful history.
  
  - For instance:
    - The extensive **astronomical observations** of **Tycho Brahe** in the 16<sup>th</sup> century allowed **Johannes Kepler** to discover the empirical laws of planetary motion, which in turn provided a springboard for the development of classical mechanics.
    - Similarly, the discovery of **regularities in atomic spectra** played a key role in the development and verification of quantum physics in the early 20<sup>th</sup> century.

  - The field of **pattern recognition** is concerned with:
    - The automatic discovery of regularities in data through the use of computer algorithms.
    - Using these regularities to take actions, such as classifying the data into different categories.

    </div> <!-- /notepad-zone -->
  </div> <!-- /body-container -->
</section>

---

<section>
  <div class="body-container" markdown="1">
    <div class="notepad-zone" markdown="1">

  ## Recognizing Handwritten Digits

  <div class="columns">

  <!-- Left Column -->
  <div class="column left-col" style="flex: 1;" markdown="1">

  - Consider the example of recognizing handwritten digits, illustrated in **Figure 1.1**.
    - Each digit corresponds to a **28 × 28 pixel image** and can be represented by a vector $\mathbf{x}$ comprising 784 real numbers.
    - The goal is to build a machine that:
      - Takes $\mathbf{x}$ as input.
      - Produces the identity of the digit ($0, \dots, 9$) as the output.

    </div>

    <!-- Right Column -->
  <div class="column right-col" style="flex: 1; text-align: center;">

  - This is a **nontrivial problem** due to the wide variability in handwriting:
    - Handcrafted rules or heuristics for distinguishing the digits based on the shapes of the strokes often lead to a **proliferation of rules and exceptions**, resulting in **poor outcomes**.

    <div class="cute-image-container">
    <!-- The push pin element (instead of tape) -->
    <div class="push-pin"></div>

    <!-- Your image and caption as usual -->
    <img src="imgs/1.png" alt="Handwritten Digits Examples" width="250"/>
    <div class="small-caption">
  
    Examples of handwritten digits taken from US zip codes.
    </div>
    </div>
  </div>
  </div> <!-- /.columns -->

</div> <!-- /notepad-zone -->
</div> <!-- /body-container -->
</section>

---

<section>
  <div class="body-container" markdown="1">
    <div class="notepad-zone" markdown="1">

  ## Training and Generalization

  <div class="columns">
  <!-- Left Column -->
  <div class="column left-col" style="flex: 1;" markdown="1">

- A **training set** is used to tune the parameters of an adaptive model.
  - Each digit in the training set is **individually inspected** and **hand-labeled**.
  - Categories of digits are represented by **target vectors** $\mathbf{t}$.

- The machine learning algorithm takes:
  - **Input vector $\mathbf{x}$** (new digit image).
  - Produces **output vector $\mathbf{y}$**, encoded in the same way as target vectors.

</div>

  <!-- Right Column -->
  <div class="column right-col" style="flex: 1;" markdown="1">

- **Key phases**:
  1. **Training phase**: $\mathbf{y(x)}$ is determined based on training data.
  2. **Generalization**: The model categorizes unseen examples correctly.

- **Practical challenges**:
  - Variability in handwriting makes training data only a small fraction of possible inputs.
  - Generalization is central to pattern recognition.

    </div>

  </div> <!-- /.columns -->

    </div> <!-- /notepad-zone -->
  </div> <!-- /body-container -->
</section>

---

<section>
  <div class="body-container" markdown="1">
    <div class="notepad-zone" markdown="1">

  ## Pre-Processing and Dimensionality Reduction

  <div class="columns">

  <!-- Left Column -->
  <div class="column left-col" style="flex: 1;" markdown="1">

  - **Pre-processing** transforms input variables to make the problem easier to solve:
    - For digit recognition, images are scaled and translated for consistency.
    - **Feature extraction** reduces variability across classes.

  - **Purpose**:
    - Reduces computational complexity.
    - Retains **useful discriminatory information**.

  </div>

  <!-- Right Column -->
  <div class="column right-col" style="flex: 1;" markdown="1">

  - Example: Real-time face detection:
    - Features like **average image intensity over regions** (e.g., Viola and Jones, 2004).
    - Reduces dimensions while retaining essential information.

  - **Challenges**:
    - Over-simplification can harm model accuracy.
    - Important to preserve features critical to solving the problem.

  </div>

</div> <!-- /.columns -->

  </div> <!-- /notepad-zone -->
</div> <!-- /body-container -->
</section>

---

<section>
  <div class="body-container" markdown="1">
    <div class="notepad-zone" markdown="1">

  ## Supervised and Unsupervised Learning

  - **Supervised Learning**:
    - Input vectors $\mathbf{x}$ with corresponding target vectors $\mathbf{t}$.
    - Example:
      - **Classification**: Assigning an input to one of a finite number of categories (e.g., digit recognition).
      - **Regression**: Predicting continuous variables (e.g., chemical yield prediction).

  - **Unsupervised Learning**:
    - Input vectors $\mathbf{x}$ without corresponding target values.
    - Examples:
      - **Clustering**: Discovering groups of similar examples in the data.
      - **Density Estimation**: Determining the distribution of data.
      - **Visualization**: Projecting high-dimensional data into 2D or 3D.

    </div> <!-- /notepad-zone -->
  </div> <!-- /body-container -->
</section>

---

<section>
  <div class="body-container" markdown="1">
    <div class="notepad-zone" markdown="1">

  ## Reinforcement Learning

  - Focuses on finding suitable actions to maximize a reward through trial and error.
  - Characteristics:
    - The algorithm interacts with the environment over a sequence of states and actions.
    - Rewards depend on current and subsequent actions.

  - Example:
    - **Backgammon (Tesauro, 1994)**:
      - The system learns to make moves that maximize rewards over the game.
      - Challenges:
        - Credit assignment problem: Associating moves with their outcomes.
        - Trade-off between exploration (trying new actions) and exploitation (using known high-reward actions).

    </div> <!-- /notepad-zone -->
  </div> <!-- /body-container -->
</section>

---

<section>
  <div class="body-container" markdown="1">
    <div class="notepad-zone" markdown="1">

  ## Tools and Techniques in Pattern Recognition

  - Each task requires specialized tools, but common ideas underlie all problems.
  - Goals of this chapter:
    - Introduce key concepts with simple examples.
    - Highlight foundational tools:
      - **Probability Theory**
      - **Decision Theory**
      - **Information Theory**

  - Later chapters:
    - Explore sophisticated models for real-world applications.

    </div> <!-- /notepad-zone -->
  </div> <!-- /body-container -->
</section>

---

<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Summary

<div class="columns">
<div class="column left-col" style="flex: 1;" markdown="1">

- **History**:  
  - From Kepler’s planetary motion to quantum physics, recognizing patterns drives progress.  
  - Algorithms now classify data into categories.  

- **Handwritten Digits**:  
  - Map 28×28 pixel images to digits.  
  - Challenge: Variability in handwriting.

</div>

<!-- Right Column -->
<div class="column right-col" style="flex: 1;" markdown="1">

- **Learning Types**:  
  - **Supervised**: Classification, regression.  
  - **Unsupervised**: Clustering, density estimation, visualization.  
  - **Reinforcement**: Trial-and-error learning.

- **Key Concepts**:  
  - Pre-processing simplifies inputs.  
  - Core tools: **Probability**, **Decision**, and **Information Theory**.

</div>

</div> <!-- /.columns -->

</div> <!-- /notepad-zone -->
</div> <!-- /body-container -->
</section>

---

<!-- _class: spiral-slide -->

<div class="notebook-container">
  <!-- The pink border around the white page -->
  <div class="page-border">
    <div class="sticky-tabs">
      <div class="tab-yellow"></div>
      <div class="tab-peach"></div>
    </div>
    <div class="white-page">
      <!-- Circle with the number -->
      <div class="circle-number">
        <span>02</span>
      </div>
    <h1 class="page-title">Example: Polynomial Curve Fitting</h1>
    <!-- <p class="subtitle">You can enter a subtitle here if you need it</p> -->
    </div>
</div>

---

<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Polynomial Curve Fitting - Overview

<div class="columns">

<!-- Left Column -->
<div class="column left-col" style="flex: 1;" markdown="1">

- **Objective**:
  - Predict the value of $t$ (target variable) for a given $x$ (input variable).
  - Use a **training set** comprising $N = 10$ points.

- **Data Generation**:
  - Target values generated using $t = \sin(2\pi x)$ plus random Gaussian noise.
  - Goal: Discover underlying patterns in noisy data.

</div>

<!-- Right Column -->
<div class="column right-col" style="flex: 1; text-align: center;">

<div class="cute-image-container">
<div class="push-pin"></div>
<img src="imgs/2.png" alt="Polynomial Curve Fitting Example" width="2500"/>
<div class="small-caption">

Training data (blue circles) and underlying function (green curve).
</div>
</div>

</div>

</div> <!-- /.columns -->

</div> <!-- /notepad-zone -->
</div> <!-- /body-container -->
</section>

---


<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Key Challenges and Curve Fitting

<div class="columns">

<!-- Left Column -->
<div class="column left-col" style="flex: 1;" markdown="1">

- **Challenges**:
  - Generalizing from a **finite training set** to new inputs.
  - Noise in observations introduces uncertainty.
  - Uncertainty in $t$ can arise due to:
      - Stochastic processes (e.g., radioactive decay).
      - Unobserved variability.

- **Approach**:
  - Fit the data using a polynomial function:
      $$
      y(x, \mathbf{w}) = \sum_{j=0}^M w_j x^j
      $$
  - $M$: Polynomial order, $w_j$: Coefficients.

</div>

<!-- Right Column -->
<div class="column right-col" style="flex: 1;" markdown="1">

- **Curve Fitting Steps**:
  1. Use training set $(\mathbf{x}, \mathbf{t})$ to find coefficients $\mathbf{w}$
  2. Fit a curve that minimizes error for observed data.
  3. Balance between:
      - Capturing underlying patterns.
      - Avoiding overfitting.

- **Further Topics**:
  - Decision theory and probability theory for handling uncertainty.

</div>

</div> <!-- /.columns -->

</div> <!-- /notepad-zone -->
</div> <!-- /body-container -->
</section>

---

<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Error Function for Polynomial Fitting

<div class="columns">

<!-- Left Column -->
<div class="column left-col" style="flex: 1;" markdown="1">

- Coefficients $\mathbf{w}$ are determined by minimizing an **error function**:
    $$
    E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \big(y(x_n, \mathbf{w}) - t_n\big)^2
    $$
  - Measures the misfit between predictions $y(x_n, \mathbf{w})$ and target values $t_n$.
  - Factor $\frac{1}{2}$ included for convenience.

  - Goal: Minimize $E(\mathbf{w})$ to ensure predictions match training data.

</div>

<!-- Right Column -->
<div class="column right-col" style="flex: 1; text-align: center;">

<div class="cute-image-container">
<div class="push-pin"></div>
<img src="imgs/3.png" alt="Error Function Visualization" width="600"/>
<div class="small-caption">

Geometric interpretation of the error function.
</div>
</div>

</div>

</div> <!-- /.columns -->

</div> <!-- /notepad-zone -->
</div> <!-- /body-container -->
</section>

---

<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Polynomial Order and Overfitting

<div class="columns" markdown="1">

<!-- Left Column -->
<div class="column left-col" markdown="1">

- The choice of polynomial order $M$ affects the model:
  - **Low-order polynomials** (e.g., $M = 0, 1$):
    - Poor representation of data.
  - **Moderate-order polynomial** ($M = 3$):
    - Captures the function $\sin(2\pi x)$ effectively.
  - **High-order polynomial** ($M = 9$):
    - Fits the training data exactly.
    - **Overfitting**: Poor generalization to unseen data.
  - **Model Selection**:
    - Balance between underfitting and overfitting.

</div>

<!-- Right Column -->
<div class="column right-col" style="flex: 1; text-align: center;">

<!-- Slide 15 snippet -->

<div class="cute-image-container">
  <div class="push-pin"></div>
  <img src="imgs/4.png" alt="Polynomial Orders Example" width="600" />
  <div class="small-caption">

  **Effects** of varying polynomial orders $m$.

  </div>
</div>

</div> <!-- end .column right-col -->
</div> <!-- end .columns -->

</div> <!-- end .notepad-zone -->
</div> <!-- end .body-container -->
</section>

---

<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Root-Mean-Square (RMS) Error

<div class="columns" markdown="1">

<!-- Left Column -->
<div class="column left-col" markdown="1">

- RMS error measures the misfit between predictions and target values:
  $$
  E_{RMS} = \sqrt{\frac{2E(\mathbf{w}^*)}{N}}
  $$
  - $E(\mathbf{w}^*)$: Error function (1.2) for optimal coefficients $\mathbf{w}^*$.
  - $N$: Number of data points.

- RMS error allows comparison across data sets of different sizes.

- Insight:
  - RMS error quantifies generalization performance on unseen data.

</div>

<!-- Right Column -->
<div class="column right-col" style="flex: 1; text-align: center;">

<div class="cute-image-container">
  <div class="push-pin"></div>
  <img src="imgs/5.png" alt="RMS Error Visualization" width="600" />
  <div class="small-caption">

 RMS error for training and test data across $M$.

  </div>
</div>

</div> <!-- end .column right-col -->
</div> <!-- end .columns -->

</div> <!-- end .notepad-zone -->
</div> <!-- end .body-container -->
</section>

---

<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Impact of Polynomial Order on Generalization

<div class="columns" markdown="1">

<!-- Left Column -->
<div class="column left-col" markdown="1">

- Observations for varying $M$:
  - **Low-order polynomials**:
    - High test error due to underfitting.
  - **Moderate-order polynomial** ($M = 3$):
    - Best balance between training and test errors.
  - **High-order polynomial** ($M = 9$):
    - Zero training error but high test error due to overfitting.

- Insights:
  - Test set error measures generalization to unseen data.
  - Balance required between underfitting and overfitting.

</div>

<!-- Right Column -->
<div class="column right-col" style="flex: 1;">

- **Generalization Performance**:
  - Test RMS error ($E_{RMS}$) quantifies the model's ability to predict new data.
  - Ideal range: $3 \leq M \leq 8$, which minimizes overfitting and underfitting.
  - As $M$ increases:
    - Training error decreases to zero.
    - Test error rises sharply due to overfitting.

<div class="cute-image-container" >
  <div class="push-pin"></div>
  <img src="imgs/5.png" alt="RMS Error Visualization" width="200" />
  <div class="small-caption">

  Generalization improves with $3 \leq M \leq 8$.

  </div>
</div>

</div> <!-- end .column right-col -->
</div> <!-- end .columns -->



</div> <!-- end .notepad-zone -->
</div> <!-- end .body-container -->
</section>

---

<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Coefficient Magnitudes and Polynomial Order

<div class="columns" markdown="1">

<!-- Left Column -->
<div class="column left-col" markdown="1">

- **Insight**:
  - As polynomial order $M$ increases:
    - Coefficients $\mathbf{w}^*$ increase dramatically in magnitude.
    - High-order polynomials finely tune to the data, resulting in large positive and negative values.

- **Observation**:
  - For $M = 9$, coefficients adapt closely to noise, amplifying oscillations in the fitted function.

</div>

<!-- Right Column -->
<div class="column right-col" style="flex: 1;">

<div class="cute-image-container">
  <div class="push-pin"></div>
  <div class="small-caption">
    <img src="imgs/6.png" alt="coefficient values for various m" width="600" />

Refer to **Table 1.1** for coefficient values for various $M$.
  </div>
</div>

</div> <!-- end .column right-col -->
</div> <!-- end .columns -->

</div> <!-- end .notepad-zone -->
</div> <!-- end .body-container -->
</section>

---
<section>
    <div class="body-container" markdown="1">
        <div class="notepad-zone" markdown="1">

## Effect of Data Size on Overfitting

<div class="columns" markdown="1">
<div class="column left-col" markdown="1">

  - **Key Insight**:
    - Larger data sets reduce overfitting for high-order polynomials.
    - Overfitting diminishes as data size increases.

  - **Example**:
    - **$N = 15$**: Large oscillations between data points.
    - **$N = 100$**: Function approximates $\sin(2\pi x)$ more closely.

  - **Guideline**:
    - Number of parameters should align with available data.

    </div>

    <!-- Right Column -->
<div class="column right-col" style="flex: 1; text-align: center;">
    <div class="cute-image-container">
        <div class="push-pin"></div>
        <img src="imgs/7.png" alt="Effect of Data Size on Overfitting" width="900" />
    <div class="small-caption">

Overfitting reduces with larger data sets.
</div>
</div>

</div> <!-- end .column right-col -->
</div> <!-- end .columns -->
    </div> <!-- end .notepad-zone -->
    </div> <!-- end .body-container -->
</section>

---

<section>
<div class="body-container" markdown="1">
<div class="notepad-zone" markdown="1">

## Bayesian Perspective and Model Complexity

<div class="columns" markdown="1">

<!-- Left Column -->
<div class="column left-col" markdown="1">

- **Model Complexity**:
  - Larger models require more data to generalize effectively.
  - Limiting parameters based on data size may not always be optimal.

- **Bayesian Approach**:
  - Adapts model complexity to the problem being solved.
  - Avoids overfitting by incorporating prior knowledge.

</div>

<!-- Right Column -->
<div class="column right-col" markdown="1">

- **Advantages**:
  - Automatically adjusts the effective number of parameters.
  - Provides a flexible framework for tackling overfitting.

</div> <!-- end .column right-col -->
</div> <!-- end .columns -->

</div> <!-- end .notepad-zone -->
</div> <!-- end .body-container -->
</section>

---